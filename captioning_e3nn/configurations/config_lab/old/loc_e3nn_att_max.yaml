preprocessing:
  mask: True
  collate_fn: masks
training_params:
  mode: attention
  log_step: 1
  save_step: 1
  encoder_name: best_test/encoder-0-32-500.ckpt
  decoder_name: best_test/decoder-0-32-500.ckpt
  n_splits: 5
model:
  encoder: network1
  encoder_kwargs:
    max_rad: 2
    num_basis: 50
    n_neurons: 80
    n_layers: 2
    beta: 5
    rad_model: gaussian
    num_embeddings: 6
    embed: 10
    l0: 10
    L: 2
    scalar_act_name:  sp
    gate_act_name: sigmoid
    natoms: 286
    mlp_h: 50
    Out: 1
    aggregation_mode: avg

  decoder: lstm_attention
  decoder_kwargs:
    attention_dim: 100
    embed_dim: 100
    decoder_dim: 100
    vocab_size: 32
    vocab_path: ../data/vocab.pkl
    encoder_dim: 100
    dropout: 0.9
    
  

model_params: 
  model_name: "e3nn"
  num_epochs: 1
  batch_size: 4
  n_splits: 5
  num_workers: 1
  learning_rate: 0.001
  input_channels: 22
  feature_type: with_labels
  representations: [(23,), (2, 2), (4, 4), (10,)]


output_parameters: 
  savedir: ../results/captioning_results/e3nn_attention_1


sampling_params: 
  sampling: max
  sample_id: 15
  number_smiles: 1
  time_waiting: 60
  type_fold: O
  id_fold: 0
  folds: ../results/captioning_results/e3nn_1/logs/idx/test_idx_0
  file_stat: 0_fold_stat.csv
  sampling_data: test
  name_all_stat: all_stat_train.csv