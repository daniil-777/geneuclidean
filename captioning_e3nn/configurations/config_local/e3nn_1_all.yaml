preprocessing:
  mask: True
  collate_fn: masks
  vocab_path: /Volumes/Ubuntu/research_drugs/data/vocab.pkl
training_params:
  log_step: 10
  save_step: 1
  encoder_name: encoder-0-1-2.ckpt
  decoder_name: decoder-0-1-2.ckpt
  n_splits: 5

model:
  encoder: se3cnn
  encoder_kwargs:
    nclouds: 2
    emb_dim: 10
    neighborradius: 2
    cloudord: 1
    cloud_dim: 5
    natoms: 286
    nffl: 1
    ffl1size: 512
    num_embeddings: 6
    nradial: 3
    nbasis: 3
    rad_neurons: 100
  decoder: lstm
  decoder_kwargs:
    vocab_size: 32
    embed_size: 20
    hidden_size: 20
    num_layers: 1


model_params: 
  model_name: "e3nn"
  num_epochs: 1
  batch_size: 4
  n_splits: 5
  num_workers: 1
  learning_rate: 0.001
  input_channels: 22
  feature_type: with_labels
  representations: [(23,), (2, 2), (4, 4), (10,)]


output_parameters: 
  savedir: /Volumes/Ubuntu/research_drugs/data/e3nn_1_test


sampling_params: 
  sampling: max
  sample_id: 15
  number_smiles: 1
  time_waiting: 2
  type_fold: O
  id_fold: 0
  folds: ../results/captioning_results/e3nn_1/logs/idx/test_idx_0
  file_stat: 0_fold_stat.csv
  sampling_data: train
  name_all_stat: stat_e3nn_all.csv